iteration,model,framework,device,pretrain_time(s),input_size,infer_count,generation_time(s),output_size,latency(ms),1st_latency(ms),2nd_avg_latency(ms),precision,max_rss_mem(MB),max_uss_mem(MB),max_shared_mem(MB),prompt_idx,1st_infer_latency(ms),2nd_infer_avg_latency(ms),num_beams,batch_size,tokenization_time,detokenization_time,result_md5,vision_latency(ms),sampler_latency(ms)
0,llama-2-7b-chat,ov,gpu,6.78373,8,512,12.6451,512,24.69747,152.72997,24.41268,unknown,4948.67188,659.14062,4289.53125,0,148.73509,24.02764,1,1,0.82467,0.61986,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
1,llama-2-7b-chat,ov,gpu,,8,512,12.65977,512,24.72611,38.38934,24.68495,unknown,,,,0,38.12571,24.3031,1,1,0.23377,0.56151,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
2,llama-2-7b-chat,ov,gpu,,8,512,12.64803,512,24.70319,36.03285,24.66718,unknown,,,,0,35.77548,24.28846,1,1,0.24419,0.54126,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
avg,llama-2-7b-chat,ov,gpu,,8.0,512.0,12.6539,512.0,24.71465,37.2111,24.67606,unknown,,,,0,36.95059,24.29578,1,1,0.23898,0.55139,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
mini,llama-2-7b-chat,ov,gpu,,8,512,12.64803,512,24.70319,36.03285,24.66718,unknown,,,,0,35.77548,24.28846,1,1,0.23377,0.54126,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
median,llama-2-7b-chat,ov,gpu,,8.0,512.0,12.6539,512.0,24.71465,37.2111,24.67606,unknown,,,,0,36.95059,24.29578,1,1,0.23898,0.55139,['4814d7f5dd0b45a3c5269b796ac92ba0'],,
,,,,,,,,,,,,,,,,,,,,,,,,,
input_size: Input token size,,,,,,,,,,,,,,,,,,,,,,,,,
output_size: Text/Code generation models: generated text token size,,,,,,,,,,,,,,,,,,,,,,,,,
infer_count: Limit the Text/Code generation models' output token size,,,,,,,,,,,,,,,,,,,,,,,,,
latency: Text/Code generation models: ms/token. Output token size / generation time,,,,,,,,,,,,,,,,,,,,,,,,,
1st_latency: Text/Code generation models: Fisrt token latency,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_avg_latency: Text/Code generation models: Other tokens (exclude first token) latency,,,,,,,,,,,,,,,,,,,,,,,,,
1st_infer_latency: Text/Code generation models: Fisrt inference latency,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_infer_avg_latency: Text/Code generation models: Other inferences (exclude first inference) latency,,,,,,,,,,,,,,,,,,,,,,,,,
result_md5: MD5 of generated text,,,,,,,,,,,,,,,,,,,,,,,,,
prompt_idx: Index of prompts,,,,,,,,,,,,,,,,,,,,,,,,,
tokenization_time: Tokenizer encode time,,,,,,,,,,,,,,,,,,,,,,,,,
detokenization_time: Tokenizer decode time,,,,,,,,,,,,,,,,,,,,,,,,,
pretrain_time: Total time of load model and compile model,,,,,,,,,,,,,,,,,,,,,,,,,
generation_time: Time for one interaction. (e.g. The duration of  answering one question or generating one picture),,,,,,,,,,,,,,,,,,,,,,,,,
iteration=0: warm-up; iteration=avg: average (exclude warm-up);iteration=mini: minimum value (exclude warm-up);iteration=median: median value (exclude warm-up);,,,,,,,,,,,,,,,,,,,,,,,,,
max_rss_mem: max rss memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,
max_shared_mem: max shared memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,
