iteration,model,framework,device,pretrain_time(s),input_size,infer_count,generation_time(s),output_size,latency(ms),1st_latency(ms),2nd_avg_latency(ms),precision,max_rss_mem(MB),max_uss_mem(MB),max_shared_mem(MB),prompt_idx,1st_infer_latency(ms),2nd_infer_avg_latency(ms),num_beams,batch_size,tokenization_time,detokenization_time,result_md5,start,end,vision_latency(ms),sampler_latency(ms)
0,MiniCPM-V-2_6,ov,CPU,2.11022,0,0,33.83507,49,690.51168,NA,NA,,29821.45312,18827.39062,10994.0625,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],2024-11-08T18:40:29.176197,2024-11-08T18:41:03.332332,,
1,MiniCPM-V-2_6,ov,CPU,,0,0,28.15491,49,574.58997,NA,NA,,,,,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],2024-11-08T18:41:03.332642,2024-11-08T18:41:31.661069,,
2,MiniCPM-V-2_6,ov,CPU,,0,0,27.86826,49,568.74005,NA,NA,,,,,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],2024-11-08T18:41:31.661093,2024-11-08T18:41:59.655260,,
avg,MiniCPM-V-2_6,ov,CPU,,0.0,0.0,28.01159,49.0,571.66501,NA,NA,,,,,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],,,,
mini,MiniCPM-V-2_6,ov,CPU,,0,0,27.86826,49,568.74005,NA,NA,,,,,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],,,,
median,MiniCPM-V-2_6,ov,CPU,,0.0,0.0,28.01159,49.0,571.66501,NA,NA,,,,,0,NA,NA,1,1,nan,nan,['8f21e91dff72bce810f3436b07eb15e4'],,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,
input_size: Input token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
output_size: Text/Code generation models: generated text token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
infer_count: Limit the Text/Code generation models' output token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
latency: Text/Code generation models: ms/token. Output token size / generation time,,,,,,,,,,,,,,,,,,,,,,,,,,,
1st_latency: Text/Code generation models: Fisrt token latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_avg_latency: Text/Code generation models: Other tokens (exclude first token) latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
1st_infer_latency: Text/Code generation models: Fisrt inference latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_infer_avg_latency: Text/Code generation models: Other inferences (exclude first inference) latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
result_md5: MD5 of generated text,,,,,,,,,,,,,,,,,,,,,,,,,,,
prompt_idx: Index of prompts,,,,,,,,,,,,,,,,,,,,,,,,,,,
vision_latency: minicpmv2 models: vision latency in multimodal input preparation,,,,,,,,,,,,,,,,,,,,,,,,,,,
sampler_latency: minicpmv2 models: sampler latency in multimodal input preparation,,,,,,,,,,,,,,,,,,,,,,,,,,,
tokenization_time: Tokenizer encode time,,,,,,,,,,,,,,,,,,,,,,,,,,,
detokenization_time: Tokenizer decode time,,,,,,,,,,,,,,,,,,,,,,,,,,,
pretrain_time: Total time of load model and compile model,,,,,,,,,,,,,,,,,,,,,,,,,,,
generation_time: Time for one interaction. (e.g. The duration of  answering one question or generating one picture),,,,,,,,,,,,,,,,,,,,,,,,,,,
iteration=0: warm-up; iteration=avg: average (exclude warm-up);iteration=mini: minimum value (exclude warm-up);iteration=median: median value (exclude warm-up);,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_rss_mem: max rss memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_shared_mem: max shared memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,,,
