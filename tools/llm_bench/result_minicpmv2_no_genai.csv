iteration,model,framework,device,pretrain_time(s),input_size,infer_count,generation_time(s),output_size,latency(ms),1st_latency(ms),2nd_avg_latency(ms),precision,max_rss_mem(MB),max_uss_mem(MB),max_shared_mem(MB),prompt_idx,1st_infer_latency(ms),2nd_infer_avg_latency(ms),num_beams,batch_size,tokenization_time,detokenization_time,result_md5,start,end,vision_latency(ms),sampler_latency(ms)
0,MiniCPM-V-2_6,ov,GPU,6.26825,361,88,3.00246,88,34.11884,150.29119,27.30977,,5890.28125,1015.39844,4874.88281,0,149.12994,25.94449,1,1,40.75269,0.34946,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],2024-11-08T18:39:55.651495,2024-11-08T18:39:58.701364,234.76361,85.52383
1,MiniCPM-V-2_6,ov,GPU,,361,88,2.85946,88,32.49388,99.65468,26.81743,,,,,0,98.85281,25.46177,1,1,36.55051,0.48331,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],2024-11-08T18:39:58.701412,2024-11-08T18:40:01.599525,196.83148,35.47538
2,MiniCPM-V-2_6,ov,GPU,,361,88,2.75041,88,31.25464,111.88363,26.27172,,,,,0,110.86209,25.33463,1,1,52.95355,0.38774,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],2024-11-08T18:40:01.599627,2024-11-08T18:40:04.405877,194.69747,38.08949
avg,MiniCPM-V-2_6,ov,GPU,,361.0,88.0,2.80493,88.0,31.87426,105.76916,26.54458,,,,,0,104.85745,25.3982,1,1,44.75203,0.43553,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],,,196.83148,35.47538
mini,MiniCPM-V-2_6,ov,GPU,,361,88,2.75041,88,31.25464,99.65468,26.27172,,,,,0,98.85281,25.33463,1,1,36.55051,0.38774,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],,,196.83148,35.47538
median,MiniCPM-V-2_6,ov,GPU,,361.0,88.0,2.80493,88.0,31.87426,105.76916,26.54458,,,,,0,104.85745,25.3982,1,1,44.75203,0.43553,['6f2aea7958c88bfcf4ec6de5f7e73c1e'],,,196.83148,35.47538
,,,,,,,,,,,,,,,,,,,,,,,,,,,
input_size: Input token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
output_size: Text/Code generation models: generated text token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
infer_count: Limit the Text/Code generation models' output token size,,,,,,,,,,,,,,,,,,,,,,,,,,,
latency: Text/Code generation models: ms/token. Output token size / generation time,,,,,,,,,,,,,,,,,,,,,,,,,,,
1st_latency: Text/Code generation models: Fisrt token latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_avg_latency: Text/Code generation models: Other tokens (exclude first token) latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
1st_infer_latency: Text/Code generation models: Fisrt inference latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
2nd_infer_avg_latency: Text/Code generation models: Other inferences (exclude first inference) latency,,,,,,,,,,,,,,,,,,,,,,,,,,,
result_md5: MD5 of generated text,,,,,,,,,,,,,,,,,,,,,,,,,,,
prompt_idx: Index of prompts,,,,,,,,,,,,,,,,,,,,,,,,,,,
vision_latency: minicpmv2 models: vision latency in multimodal input preparation,,,,,,,,,,,,,,,,,,,,,,,,,,,
sampler_latency: minicpmv2 models: sampler latency in multimodal input preparation,,,,,,,,,,,,,,,,,,,,,,,,,,,
tokenization_time: Tokenizer encode time,,,,,,,,,,,,,,,,,,,,,,,,,,,
detokenization_time: Tokenizer decode time,,,,,,,,,,,,,,,,,,,,,,,,,,,
pretrain_time: Total time of load model and compile model,,,,,,,,,,,,,,,,,,,,,,,,,,,
generation_time: Time for one interaction. (e.g. The duration of  answering one question or generating one picture),,,,,,,,,,,,,,,,,,,,,,,,,,,
iteration=0: warm-up; iteration=avg: average (exclude warm-up);iteration=mini: minimum value (exclude warm-up);iteration=median: median value (exclude warm-up);,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_rss_mem: max rss memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_shared_mem: max shared memory consumption;,,,,,,,,,,,,,,,,,,,,,,,,,,,
